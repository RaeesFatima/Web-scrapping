# -*- coding: utf-8 -*-
"""medicines.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17CEYihdlrCDfIo-SLKZw2ugtXc2MEd8U
"""

!pip install selenium

!pip install webdriver-manager

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
options = Options()
options.add_argument('--headless')  # Run in headless mode
options.add_argument('--no-sandbox')  # Required for Colab
options.add_argument('--disable-dev-shm-usage')  # Overcome limited resources
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb
!apt-get -f install -y
!which google-chrome

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

# Set up Chrome in headless mode
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.binary_location = "/usr/bin/google-chrome"  # Explicitly set the Chrome binary location

# Initialize WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

def scrape_medicines():
    # Scrape the list of medicine details
    medicines = driver.find_elements(By.CSS_SELECTOR, 'a.show-loader')
    for medicine in medicines:
        try:
            # Attempt to extract details
            name = medicine.find_element(By.CSS_SELECTOR, 'span.name').text
            price = medicine.find_element(By.CSS_SELECTOR, 'span.price').text
            image_url = medicine.find_element(By.TAG_NAME, 'img').get_attribute('src')

            # Print or save the data
            print(f"Name: {name}, Price: {price}, Image URL: {image_url}")
        except NoSuchElementException as e:
            print(f"Skipping medicine due to missing element: {e}")

def scrape_a_to_z():
    # Scrape data for each letter A to Z
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        url = f"https://healthwire.pk/pharmacy/medicines/{letter}"
        print(f"Scraping data for letter: {letter} ({url})")

        driver.get(url)
        sleep(2)  # Wait for the page to load

        # Scrape the medicine data
        scrape_medicines()

try:
    scrape_a_to_z()
finally:
    driver.quit()

import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

# Set up Chrome in headless mode
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.binary_location = "/usr/bin/google-chrome"  # Explicitly set the Chrome binary location

# Initialize WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Define CSV file path
csv_file = '/content/medicines_data.csv'

# Write headers to CSV file
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Letter', 'Name', 'Price', 'Image URL'])  # Added 'Letter' column

def scrape_medicines(letter):
    # Scrape the list of medicine details
    medicines = driver.find_elements(By.CSS_SELECTOR, 'a.show-loader')
    for medicine in medicines:
        try:
            # Attempt to extract details
            name = medicine.find_element(By.CSS_SELECTOR, 'span.name').text
            # Extract price and avoid `del` tag if present
            price_element = medicine.find_element(By.CSS_SELECTOR, 'span.price')
            price = price_element.text if not price_element.find_element(By.TAG_NAME, 'del') else price_element.text.replace(price_element.find_element(By.TAG_NAME, 'del').text, '').strip()

            # Get image URL
            image_url = medicine.find_element(By.TAG_NAME, 'img').get_attribute('src')

            # Print or save the data with the current letter
            with open(csv_file, mode='a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                writer.writerow([letter, name, price, image_url])  # Write row with letter data
        except NoSuchElementException as e:
            print(f"Skipping medicine due to missing element: {e}")

def scrape_a_to_z():
    # Scrape data for each letter A to Z
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        url = f"https://healthwire.pk/pharmacy/medicines/{letter}"
        print(f"Scraping data for letter: {letter} ({url})")

        driver.get(url)
        sleep(2)  # Wait for the page to load

        # Scrape the medicine data with the current letter
        scrape_medicines(letter)

try:
    scrape_a_to_z()
finally:
    driver.quit()

import csv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from time import sleep
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException

# Set up Chrome in headless mode
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.binary_location = "/usr/bin/google-chrome"  # Explicitly set the Chrome binary location

# Initialize WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Define CSV file path
csv_file = '/content/medicines_data.csv'

# Write headers to CSV file
with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(['Letter', 'Name', 'Price', 'Image URL', 'Page Number'])  # Added 'Page Number' column

def scrape_medicines(letter, page_number):
    # Scrape the list of medicine details
    medicines = driver.find_elements(By.CSS_SELECTOR, 'a.show-loader')
    for medicine in medicines:
        try:
            # Attempt to extract details
            name = medicine.find_element(By.CSS_SELECTOR, 'span.name').text
            # Extract price and avoid `del` tag if present
            price_element = medicine.find_element(By.CSS_SELECTOR, 'span.price')
            price = price_element.text if not price_element.find_element(By.TAG_NAME, 'del') else price_element.text.replace(price_element.find_element(By.TAG_NAME, 'del').text, '').strip()

            # Get image URL
            image_url = medicine.find_element(By.TAG_NAME, 'img').get_attribute('src')

            # Save the data with the current letter and page number
            with open(csv_file, mode='a', newline='', encoding='utf-8') as file:
                writer = csv.writer(file)
                writer.writerow([letter, name, price, image_url, page_number])  # Write row with letter and page number
        except NoSuchElementException as e:
            print(f"Skipping medicine due to missing element: {e}")

def scrape_a_to_z():
    # Scrape data for each letter A to Z
    for letter in 'abcdefghijklmnopqrstuvwxyz':
        page_number = 1  # Start at page 1 for each letter
        url = f"https://healthwire.pk/pharmacy/medicines/{letter}"
        print(f"Scraping data for letter: {letter} ({url})")

        driver.get(url)
        sleep(2)  # Wait for the page to load

        # Scrape the first page
        scrape_medicines(letter, page_number)

        # Load more and scrape additional pages until no more pages are found
        while True:
            try:
                # Locate and wait until the "Load More" button is clickable
                load_more_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CLASS_NAME, 'load-more-btn'))
                )

                # Scroll to the "Load More" button if necessary
                ActionChains(driver).move_to_element(load_more_button).perform()

                # Click the "Load More" button
                load_more_button.click()
                sleep(2)  # Wait for the new medicines to load
                page_number += 1
                print(f"Scraping page {page_number} for letter {letter}")

                # Scrape the medicines for the current page
                scrape_medicines(letter, page_number)
            except NoSuchElementException:
                print(f"No more pages for letter {letter} or error occurred.")
                break  # Stop if no more "Load More" button is found
            except ElementClickInterceptedException:
                print(f"Click intercepted for letter {letter}, trying again.")
                sleep(1)  # Wait for a while and retry clicking

try:
    scrape_a_to_z()
finally:
    driver.quit()

from google.colab import files
files.download('/content/medicines_data.csv')

from git import Repo

repo_url = 'https://github.com/RaeesFatima/Web-scrapping.git'
local_dir = '/content/Web-scrapping'

# Clone the repository
!git clone {repo_url} {local_dir}

import shutil

# Create 'Medicines' folder if not exists
!mkdir -p /content/Web-scrapping/Medicines

shutil.move('/content/medicines_data.csv', '/content/Web-scrapping/Medicines/medicines_data.csv')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/Web-scrapping

!git add .

!git config --global user.email "raeesyousaf555@gmail.com"
!git config --global user.name "RaeesFatima"

!git commit -m "medicines data csv"

!git push https://github.com/RaeesFatima/Web-scrapping.git main

